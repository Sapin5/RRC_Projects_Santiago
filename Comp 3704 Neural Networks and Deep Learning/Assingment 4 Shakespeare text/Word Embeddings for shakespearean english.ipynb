{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR4fiRdFXI7W",
        "outputId": "358faa90-fa56-41b4-d2e1-03a7cba5aefd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/622.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/622.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m614.4/622.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622363 sha256=39778c9cd6721d1da9e4a2e75732806be8794495ee9f0d0c416a9de67d43d8b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/7b/6d/b76b29ce11ff8e2521c8c7dd0e5bfee4fb1789d76193124343\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n"
          ]
        }
      ],
      "source": [
        "pip install autocorrect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUFPPW0rtf_I",
        "outputId": "961c629f-e1c2-4e96-d3e5-743aac38652a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhPNIxetPGOJ",
        "outputId": "c81bdaca-47a4-4692-afab-0cc78064d347"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-10-20 14:05:27--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-10-20 14:05:27--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-10-20 14:05:27--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2023-10-20 14:08:07 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-whS0WZDTw8U",
        "outputId": "ffdc214b-edaf-427a-94a0-f163856f4d61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n",
            "glove.6B.200d.txt  glove.6B.50d.txt   sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvlRt7WVeEzF"
      },
      "source": [
        "#Assignment 4: Word Embeddings for Shakespearean English\n",
        "## COMP-3704 (241552) Neural Networks and Deep Learning\n",
        "\n",
        "###Brandon, Santiago"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxicv-B2eOhO"
      },
      "source": [
        "## Objective\n",
        "To train word embeddings on famous works of Shakespeare and evaluate their understanding.\n",
        "\n",
        "##Data\n",
        "The entire text of plays: 1) The Tragedy of Hamlet, Prince of Denmark, 2) The Tragedy of Macbeth, and 3) The Tragedy of Julius Caesar. These are available from the Gutenberg corpus of the NLTK library. Characters and synopses can be found on Wikipedia.\n",
        "\n",
        "##Problem statement\n",
        "Natural language processing is an important part of the most advanced artificial intelligence software we have today. By studying volumes of text, word embeddings are able to elicit meaning from the words within training data. Your goal is to train a word embedding on three famous works of Shakespeare to determine how well your embedding can understand the meaning of character names and other Shakespearean English words found in these plays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g44n6qGCdl0"
      },
      "source": [
        "## 1. Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFcbtl877B-T",
        "outputId": "6a6388c6-b896-485c-c8ec-17406c8e7cde"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# Importing Libraries\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# download nltk packs, collab requires these be downloaded every run\n",
        "nltk.download('stopwords')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Importing specific functions\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "from nltk import tokenize\n",
        "from nltk.util import flatten\n",
        "from nltk.lm import vocabulary\n",
        "\n",
        "\n",
        "from gensim.scripts import glove2word2vec\n",
        "from pprint import pprint\n",
        "from gensim.models import Word2Vec, keyedvectors\n",
        "from autocorrect import Speller\n",
        "from tabulate import tabulate, SEPARATING_LINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE5DHddAefTr"
      },
      "source": [
        "###Creating stop words and importing plays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uouaPjf5wa_Y"
      },
      "outputs": [],
      "source": [
        "# Importing three plays\n",
        "plays = [(gutenberg.raw('shakespeare-hamlet.txt')),\n",
        "         (gutenberg.raw('shakespeare-macbeth.txt')),\n",
        "         (gutenberg.raw('shakespeare-caesar.txt'))]\n",
        "\n",
        "# Downloading stop word list from nltk\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Added stop words from old english\n",
        "stp_words = ['thy', 'thou', 'thee', 'thine', 'doth',\n",
        "             'shalt', 'wert', 'wilt', 'oft', 'art',\n",
        "             'like', 'would', 'know', 'make', 'hath',\n",
        "             'ith', 'hast', 'though', 'oh', 'th', 'thats',\n",
        "             'k', 'ii', 'whos', 'ther', 'tis', 'may', 'let',\n",
        "             'therefore', \"much\", 'whose', 'made', 'shall',\n",
        "             'come', 'vs','h', 'must', 'ought', 'qu', 'em','thi',\n",
        "             'seems', 'theres', 'g', 'mee', 'al']\n",
        "\n",
        "# Combining stop words\n",
        "stp_words.extend(stop_words)\n",
        "\n",
        "# Importing stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Setting autocorrect to english and making it do it fast\n",
        "check = Speller(lang = 'en', fast = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX-tWsKjek-Z"
      },
      "source": [
        "### Cleansing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gB0nTN0yK-fk"
      },
      "outputs": [],
      "source": [
        "# Tokenizing paragraphs to sentances, and setting sentances to lowercase\n",
        "temp = flatten([tokenize.sent_tokenize(i.lower()) for i in plays])\n",
        "# Tokenizes sentances to words\n",
        "# Check is autocorrect, auto corrects current word\n",
        "# stemmer.stem stems current word\n",
        "# r'[^\\w\\s]', '' - removes punctuation\n",
        "temp = [tokenize.word_tokenize(check(\n",
        "                  stemmer.stem(re.sub(r'[^\\w\\s]', '',\n",
        "                  i)))) for i in temp]\n",
        "\n",
        "# Removes StopWords\n",
        "temp = [[i for i in b if i not in stp_words] for b in temp]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKnmi5lT-FYx",
        "outputId": "d45bedae-c222-45da-da45-faed754c57d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['tragedies', 'hamlet', 'william', 'shakespeare', '1599', 'acts', 'prime']\n",
            "['scoena', 'prima']\n",
            "['enter', 'barnardo', 'francisco', 'two', 'sentinel']\n",
            "['barnardo']\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "# Print first 5 sentances\n",
        "i =0\n",
        "while i < 5:\n",
        "  print(temp[i])\n",
        "  i+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG23G8lxstCh"
      },
      "source": [
        "By printing out the first 5 sentance, we were able to find a couple more words that could have been considered stop words in shakespear english. These were added to the list of stop words to remove"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7PdnknGhWwx"
      },
      "source": [
        "## 2. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot43xMJvepd1"
      },
      "source": [
        "### Function to display top 20 words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oreMrRI_6qB"
      },
      "outputs": [],
      "source": [
        "# Function prints top 20 words\n",
        "def top_20(model):\n",
        "\n",
        "  # Saves word rank, word and frequency\n",
        "  tab = [[count+1, word, model.wv.get_vecattr(word, 'count')] \\\n",
        "         # Enumerates list of words from index_to_key\n",
        "         for count, word in enumerate(model.wv.index_to_key)\\\n",
        "         # Ensures only first 20 are used\n",
        "         if count < 20]\n",
        "  # Header names\n",
        "  col_names = ['Rank', 'Word', 'Frequency']\n",
        "\n",
        "  # prints tabulated results\n",
        "  print(tabulate(tab, headers=col_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q07uqAxVewRF"
      },
      "source": [
        "### Creating models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29rnLXN1jS5W",
        "outputId": "657a45c7-8ddd-4b3c-e2ba-c3f41a73f589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Rank  Word      Frequency\n",
            "------  ------  -----------\n",
            "     1  ham             337\n",
            "     2  lord            300\n",
            "     3  enter           230\n",
            "     4  good            218\n",
            "     5  mac             205\n",
            "     6  cesar           193\n",
            "     7  well            173\n",
            "     8  one             166\n",
            "     9  von             155\n",
            "    10  bru             153\n",
            "    11  brutus          147\n",
            "    12  self            140\n",
            "    13  king            139\n",
            "    14  go              138\n",
            "    15  yet             136\n",
            "    16  man             134\n",
            "    17  speak           130\n",
            "    18  say             128\n",
            "    19  see             122\n",
            "    20  give            118\n"
          ]
        }
      ],
      "source": [
        "# make the CBOW Word2Vex model\n",
        "cbow_model = Word2Vec(\n",
        "            sentences=temp,\n",
        "            vector_size= 100,\n",
        "            epochs= 20,\n",
        "            window= 9,\n",
        "            min_count= 5,\n",
        "            sg= 0,\n",
        "            workers = 4)\n",
        "\n",
        "# save the model to a file\n",
        "cbow_model.save('cbow_word2vec.model')\n",
        "\n",
        "# loading the model\n",
        "cbow_model = Word2Vec.load('cbow_word2vec.model')\n",
        "\n",
        "# Displaying 20 most frequent words\n",
        "top_20(cbow_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3Ad7xkxtrCh"
      },
      "source": [
        "From this list of top 20 words we have added a few to words to remove since they were words that did not add much context or meaning to sentances. Such as \"vs\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km9zHIdz9IsR"
      },
      "outputs": [],
      "source": [
        "# make the skipgram Word2Vex model\n",
        "skipgram_model = Word2Vec(\n",
        "            sentences=temp,\n",
        "            vector_size= 70,\n",
        "            epochs= 20,\n",
        "            window= 3,\n",
        "            min_count= 5,\n",
        "            sg= 1,\n",
        "            workers = 6\n",
        ")\n",
        "\n",
        "# save the model to a file\n",
        "skipgram_model.save('skipgram_word2vec.model')\n",
        "\n",
        "# loading the model\n",
        "skipgram_model = Word2Vec.load('skipgram_word2vec.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tZljK2-RQ5u"
      },
      "outputs": [],
      "source": [
        "glove_model = keyedvectors.load_word2vec_format('glove.6B.50d.txt',\n",
        "                                                binary = False,\n",
        "                                                no_header = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhDvJ2rdd48r"
      },
      "source": [
        "### GloVe model training\n",
        "\n",
        "The GloVe model have beeen trained on the non-zero entries of a aggregated global word-word co-occurence matrix. This matrix tabulates how frequently words co-occur with one another in a given corpus\n",
        "\n",
        "To populate the matrix we are required to do a single pass through the entire corpus, which is computationally expensive. But it is a one time cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-81IDMTehPK"
      },
      "source": [
        "## 3. Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7XcoSXue2EE"
      },
      "source": [
        "### 3A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tzHAfMk5yJa",
        "outputId": "b3ba49d0-b8e2-48c7-a2b6-f074d9e92b38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similar terms to 'hamlet':\n",
            "CBOW     : queen, king, laertes, polonius, poloniu\n",
            "Skip-gram: queen, laertes, polonius, horatio, king\n",
            "GloVe    : village, situated, town, obec, sleepy\n",
            "\n",
            "Similar terms to 'cauldron':\n",
            "CBOW     : double, trouble, fire, burned, scale\n",
            "Skip-gram: burned, appear, wisely, demand, directly\n",
            "GloVe    : caldron, flame, flickering, brominated, cauldrons\n",
            "\n",
            "Similar terms to 'nature':\n",
            "CBOW     : still, might, even, stop, ore\n",
            "Skip-gram: breath, oth, purpose, mortal, business\n",
            "GloVe    : context, particular, aspects, aspect, unique\n",
            "\n",
            "Similar terms to 'spirit':\n",
            "CBOW     : fell, begin, yes, lay, swords\n",
            "Skip-gram: power, motion, strength, sting, country\n",
            "GloVe    : passion, desire, faith, essence, belief\n",
            "\n",
            "Similar terms to 'general':\n",
            "CBOW     : petty, capitol, base, hearts, begin\n",
            "Skip-gram: wish, dangerous, hes, spoke, change\n",
            "GloVe    : chief, secretary, vice, deputy, appointed\n",
            "\n",
            "Similar terms to 'prythee':\n",
            "CBOW     : cheese, mothers, grown, memory, fellow\n",
            "Skip-gram: strata, humbly, thank, tit, presently\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# list the terms to find similar words later\n",
        "terms = ['hamlet', 'cauldron', 'nature', 'spirit', 'general', 'prythee']\n",
        "\n",
        "# find the most similar terms for each model\n",
        "def most_similar_terms(models, term):\n",
        "  result = {}\n",
        "  for word, model in models.items():\n",
        "    if model != glove_model:\n",
        "      similar_terms = [similar for similar, _ in model.wv.most_similar(term, topn = 5)]\n",
        "      result[word] = similar_terms\n",
        "\n",
        "    elif model == glove_model and term != 'prythee':\n",
        "      similar_terms = [similar for similar, _ in model.most_similar(term, topn = 5)]\n",
        "      result[word] = similar_terms\n",
        "  return result\n",
        "\n",
        "# models to compare\n",
        "models = {\n",
        "    'CBOW     ': cbow_model,\n",
        "    'Skip-gram': skipgram_model,\n",
        "    'GloVe    ': glove_model\n",
        "}\n",
        "\n",
        "# find and display similar terms\n",
        "for term in terms:\n",
        "  print(f\"Similar terms to '{term}':\")\n",
        "  similar_terms = most_similar_terms(models, term)\n",
        "  for word, terms in similar_terms.items():\n",
        "    print(f\"{word}: {', '.join(terms)}\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9B5cDWH8IWF"
      },
      "source": [
        "#### 'Hamlet'\n",
        "Both cbow and skipgram appear to have captured people close to hamlet, and the fast that he was up for the throne so he would have been king. But we do not believe it captured the meaning of the word overall\n",
        "\n",
        "Glove gave what appears to be random words related to the land/area hamlet took place since it gives us words such as village and town\n",
        "\n",
        "#### 'Cauldron'\n",
        "Cbow model has appeared to capture the attributes of a cauldren. Since it can burn things, witches use it to cause trouble and it requires a fire to use, and the scale of what you produce can be changed. overall it did not really capute the meaning of the word\n",
        "\n",
        "skipgram also appears to have caputred some attributes of a cauldron but not the meaning\n",
        "\n",
        "GloVe model appears to have captured best what a cauldron is, the words it said are most similar are good descriptioins of a caudron\n",
        "\n",
        "#### 'Nature'\n",
        "We believe cbow has captured a decent meaning of the word. Nature is still, and assuming that it is referring to the might of it, it has much might, it is even to all that live in it with its cycles of life and death\n",
        "\n",
        "Skipgram model did capture the meaning of nature if you consider nature evil, oth means wicked, and it can be wicked with or without a purpose, and mortals live within it\n",
        "\n",
        "GloVe model did not capture any meaning of nature whatsoever\n",
        "\n",
        "##### 'Spirit'\n",
        "We belive that the meaning of spirit was not captured by Cbow. Fell could be seen as felled but that refers to a person dying not a spirit, Begin could be considered since in macbeth spirits usually start some sort of trouble but the other 3 similar words just dont make sense\n",
        "\n",
        "Skipgram captured more of a meaning for the word spirit. Spirits in the story have some level of power and strength, there presence alone scares people who see them. Seeing a spirit in the story usually results in something beinf set into motion.\n",
        "\n",
        "GloVe model captured more if a joyous/insightful meaning to the word spirit.\n",
        "\n",
        "#### 'General'\n",
        "In the case of the stories, cbow captured the meaning is general fairly well. The generals tend to be petty, begin trouble, and represent the capitol to an extent.\n",
        "\n",
        "Skipgram also captured a good meaning for general, they are dangerous, they are all men in the story (hes_), they wish for a change of some sort\n",
        "\n",
        "Glove model agve the word general more of an overarching meaning, just what you think of when you normally think of one\n",
        "\n",
        "### 'prythee'\n",
        "both cbow and skipgram did not capture the meaning of the word prythee\n",
        "prythee is a an informal and polite way to make a request or ask someone to do something\n",
        "\n",
        "GloVe model did not conatin the word prythee so it was skipped\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG1zgoVxe_3b"
      },
      "source": [
        "### 3B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p4rcBHn03PF"
      },
      "outputs": [],
      "source": [
        "# list of cosine words\n",
        "cosine_words = [['brutus', 'murder'],\n",
        "                ['lady', 'macbeth', 'queen', 'gertrude'],\n",
        "                ['fortinbras', 'norway'],\n",
        "                ['rome', 'norway'],\n",
        "                ['ghost', 'spirit'],\n",
        "                ['macbeth', 'hamlet']]\n",
        "\n",
        "# list containing models\n",
        "models_cos = [cbow_model, skipgram_model, glove_model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bb5oIK0Ke3V"
      },
      "outputs": [],
      "source": [
        "cbow = []\n",
        "skpgram = []\n",
        "glove = []\n",
        "\n",
        "# Loops through models\n",
        "for q in range(len(models_cos)):\n",
        "\n",
        "  # Checks if the loop is on the final iterationm since the final model is\n",
        "  # the Glove model, and Glove model does not use wv\n",
        "  if q != 2:\n",
        "\n",
        "    # Loops through the cosine word list\n",
        "    for i in range(len(cosine_words)):\n",
        "\n",
        "      # one of the cosine word lists is longer than the others and\n",
        "      # requires a extra step for cosine computation\n",
        "      if len(cosine_words[i]) == 2:\n",
        "\n",
        "        # Saves first and second wv to variable\n",
        "        res1 = models_cos[q].wv[cosine_words[i][0]]\n",
        "        res2 = models_cos[q].wv[cosine_words[i][1]]\n",
        "\n",
        "      else:\n",
        "\n",
        "        # Saves all 4 wv to different vairable\n",
        "        v1 = models_cos[q].wv[cosine_words[i][0]]\n",
        "        v2 = models_cos[q].wv[cosine_words[i][1]]\n",
        "        v3 = models_cos[q].wv[cosine_words[i][2]]\n",
        "        v4 = models_cos[q].wv[cosine_words[i][3]]\n",
        "\n",
        "        # math\n",
        "        res1 = (v1+v2)/2\n",
        "        res2 = (v3+v4)/2\n",
        "\n",
        "      # Computes cosine similarities between two words\n",
        "      cosine = models_cos[q].wv.cosine_similarities(res1, [res2])\n",
        "\n",
        "      # Checks what model program is currently on\n",
        "      if models_cos[q] == cbow_model:\n",
        "\n",
        "        # Appends to cbow list for tabulation later on\n",
        "        cbow.append(cosine)\n",
        "      else:\n",
        "\n",
        "        #  Appends to skpgram list for tabulation later on\n",
        "        skpgram.append(cosine)\n",
        "  # runs same program as one above but without .wv\n",
        "  # since keyedvectors has no attribute .wv\n",
        "  else:\n",
        "\n",
        "    # Loops through\n",
        "    for i in range(len(cosine_words)):\n",
        "      # one of the cosine word lists is longer than the others and\n",
        "      # requires a extra step for cosine computation\n",
        "      if len(cosine_words[i]) == 2:\n",
        "\n",
        "        # Saves first and second wv to variable\n",
        "        res1 = models_cos[q][cosine_words[i][0]]\n",
        "        res2 = models_cos[q][cosine_words[i][1]]\n",
        "\n",
        "      else:\n",
        "\n",
        "        # Saves all 4 wv to different vairable\n",
        "        v1 = models_cos[q][cosine_words[i][0]]\n",
        "        v2 = models_cos[q][cosine_words[i][1]]\n",
        "        v3 = models_cos[q][cosine_words[i][2]]\n",
        "        v4 = models_cos[q][cosine_words[i][3]]\n",
        "\n",
        "        # Math\n",
        "        res1 = (v1+v2)/2\n",
        "        res2 = (v3+v4)/2\n",
        "      # Computes cosine similarities between two words\n",
        "      cosine = models_cos[q].cosine_similarities(res1, [res2])\n",
        "      # Appends to glove list for tabulation later on\n",
        "      glove.append(cosine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtoOJTiURrin",
        "outputId": "01c0cee9-26b6-4937-cffc-16f2efb4b333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+--------------------+---------------+-------------------+----------------+\n",
            "| First word       | Second word        |   cbow cosine |   skipgram cosine |   glove cosine |\n",
            "+==================+====================+===============+===================+================+\n",
            "| ['brutus']       | ['murder']         |      0.936144 |          0.597817 |      0.0437047 |\n",
            "+------------------+--------------------+---------------+-------------------+----------------+\n",
            "| ['lady macbeth'] | ['queen gertrude'] |      0.972087 |          0.769663 |      0.780443  |\n",
            "+------------------+--------------------+---------------+-------------------+----------------+\n",
            "| ['fortinbras']   | ['norway']         |      0.997386 |          0.96293  |     -0.0363297 |\n",
            "+------------------+--------------------+---------------+-------------------+----------------+\n",
            "| ['rome']         | ['norway']         |      0.987662 |          0.813758 |      0.267536  |\n",
            "+------------------+--------------------+---------------+-------------------+----------------+\n",
            "| ['ghost']        | ['spirit']         |      0.990786 |          0.732103 |      0.436075  |\n",
            "+------------------+--------------------+---------------+-------------------+----------------+\n",
            "| ['macbeth']      | ['hamlet']         |      0.914809 |          0.541702 |      0.480937  |\n",
            "+------------------+--------------------+---------------+-------------------+----------------+\n"
          ]
        }
      ],
      "source": [
        "# x represents the firs word\n",
        "x = [['brutus'], ['lady macbeth'], ['fortinbras'],\n",
        "     ['rome'], ['ghost'], ['macbeth']]\n",
        "\n",
        "# y represents the second word, or the word being compared against the first word\n",
        "y = [['murder'], ['queen gertrude'], ['norway'],\n",
        "     ['norway'], ['spirit'], ['hamlet']]\n",
        "\n",
        "# Merges all lists previously created for tabulation\n",
        "num = list(zip(x, y,cbow,skpgram, glove))\n",
        "\n",
        "# Headers\n",
        "headers = ['First word', 'Second word', 'cbow cosine',\n",
        "           'skipgram cosine', 'glove cosine']\n",
        "\n",
        "# Prints tabulte table\n",
        "print(tabulate(num, headers=headers, tablefmt='grid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GvUCD5OFD0S"
      },
      "source": [
        "#### Cbow\n",
        "cbow has the highest cosine between two words out of the three models. All of the cosine values are over 0.9.\n",
        "\n",
        "Considering the data is multiple plays that are similar in story seeing that characters who are also similar have high cosine is pretty impressive (macbeth and hamlet, lady macbeth and queen gertrude).\n",
        "\n",
        "The four cities all also have a high cosine which, and they are all from different plays\n",
        "\n",
        "#### skipgram\n",
        "Skipgram did not perform as well here. It gave a low cosine between brutus and murder.\n",
        "\n",
        "Hamlet and macbeth were also given a fairly low cosine, despite that they are both characters that suffer from a tragedy in their given stories maybe it is from how they are refered to in each story?\n",
        "\n",
        "Lady macbeth and queen gertrude had a fairly high cosine, which makes sense since they are both engaged/married to royalty and play pivitol roles involving the tragidy of the hero.\n",
        "\n",
        "The four cities were given the highet cosine of them all.\n",
        "\n",
        "Ghost and spirit were also given a fairly high cosine, despite the fact that they both act differently in each play. But we guess that since they are both just messangers in the story they are considered to be the same\n",
        "\n",
        "#### GloVe\n",
        "Glove just performed poorly, we believe that this is due to the fact that it was not trained on any data from the stories but rather how frequently words co appear with one another"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5pnFAdPfDo9"
      },
      "source": [
        "### 3C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB60iF5Xi7U4",
        "outputId": "8f8f63fe-bd84-42cc-e657-b71716ab8682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------------------+----------------+----------------+-------------+-------------+-------------+-------------+\n",
            "| Words to compare                | Model used     | Most similar   | 2nd place   | 3rd place   | 4th place   | 5th place   |\n",
            "|---------------------------------+----------------+----------------+-------------+-------------+-------------+-------------|\n",
            "|                                 | cbow model     | laertes        | polonius    | ophelia     | horatio     | lords       |\n",
            "| ['denmark', 'queen']            | skipgram model | laertes        | polonius    | queen       | rosincrance | ophelia     |\n",
            "|                                 | glove model    | queen          | denmark     | kingdom     | britain     | sweden      |\n",
            "| \u0001 |\n",
            "|                                 | cbow model     | yong           | ears        | soldier     | proud       | course      |\n",
            "| ['scotland', 'army', 'general'] | skipgram model | titan          | cicero      | greefes     | mor         | thank       |\n",
            "|                                 | glove model    | army           | general     | command     | force       | military    |\n",
            "| \u0001 |\n",
            "|                                 | cbow model     | sorrow         | mans        | wife        | throne      | common      |\n",
            "| ['father', 'man', 'woman']      | skipgram model | lived          | traitor     | lands       | bears       | offended    |\n",
            "|                                 | glove model    | man            | woman       | mother      | father      | boy         |\n",
            "| \u0001 |\n",
            "|                                 | cbow model     | sorrow         | mans        | break       | withall     | sodaine     |\n",
            "| ['mother', 'woman', 'man']      | skipgram model | traitor        | woman       | withall     | sales       | guilt       |\n",
            "|                                 | glove model    | woman          | mother      | man         | her         | girl        |\n",
            "| \u0001 |\n",
            "+---------------------------------+----------------+----------------+-------------+-------------+-------------+-------------+\n"
          ]
        }
      ],
      "source": [
        "linear_terms = [['denmark', 'queen'],\n",
        "                ['scotland', 'army', 'general'],\n",
        "                ['father', 'man', 'woman'],\n",
        "                ['mother', 'woman', 'man']]\n",
        "\n",
        "models_cos = [cbow_model, skipgram_model, glove_model]\n",
        "\n",
        "sim = []\n",
        "tmp = []\n",
        "track = 0\n",
        "for term in linear_terms:\n",
        "  for i in models_cos:\n",
        "    sum = 0\n",
        "    for q in term:\n",
        "\n",
        "      if i != glove_model:\n",
        "        sum+=i.wv[q]\n",
        "      else:\n",
        "        sum+=i[q]\n",
        "\n",
        "    if i == cbow_model:\n",
        "\n",
        "      tmp = i.wv.most_similar(sum, topn=5)\n",
        "      tmp_2 = ['cbow model']\n",
        "      tmp_2.insert(0, '')\n",
        "\n",
        "    elif i == skipgram_model:\n",
        "\n",
        "      tmp = i.wv.most_similar(sum, topn=5)\n",
        "      tmp_2 = ['skipgram model']\n",
        "      tmp_2.insert(0, term)\n",
        "\n",
        "    else:\n",
        "\n",
        "      tmp = i.most_similar(sum, topn =5)\n",
        "      tmp_2 = ['glove model']\n",
        "      tmp_2.insert(0, '')\n",
        "\n",
        "    for h in tmp:\n",
        "\n",
        "      (u, m) = h\n",
        "\n",
        "      tmp_2.append(u)\n",
        "\n",
        "    sim.append(tmp_2)\n",
        "    if i == glove_model:\n",
        "      sim.append(SEPARATING_LINE)\n",
        "\n",
        "\n",
        "\n",
        "headers = ['Words to compare', 'Model used', 'Most similar',\n",
        "           '2nd place', '3rd place',\n",
        "           '4th place','5th place',]\n",
        "print(tabulate(sim, headers=headers, tablefmt=\"psql\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cznJkDqIdvT"
      },
      "source": [
        "#### 'Denmark' + 'Queen'\n",
        "The Cbow model model listed the name of the royal family of denmark from the play hamlet, and horatio who is hamlets best friend. it also includes the word lords which is a good way to describe he relationship between queen and denmark. Overall it did a good job describing the idea behind the word vectors\n",
        "\n",
        "The sg model also listed the royal family, but this time instead of listing horatio, it included Rosencrantz who was close to the royal family and was tasked with spying on hamlet. It also included the word queen instead of lords. Overall it did a good job describing the idea behind the word vectors\n",
        "\n",
        "The GloVe model described where denmark was located and that it was a kingdom with. we dont think it did a good job describing the idea behind the word vectors\n",
        "\n",
        "#### 'Scotland' + 'Army' + 'General'\n",
        "The cbow model gave the words yong (youthfull), soldier, proud, course, and ears. The soldiers were all youthfull, proud and well soldiers. We would say that sbow described the idea behind the word vectors well.\n",
        "\n",
        "the sg model gave some interesting results. Titan was autocorrected by the checker, We have no idea what it was before, cicero was most likely caeser who was a roman general, greefes means sorrow or troubles which a general and army woudl experience. We have no clue what to make of thank or mor. Overall it did an ok job at describing the idea behind the word vectors.\n",
        "\n",
        "GloVe model gave words directly related to an army and generak such as command, force, and military. It did an excellent job describing the idea behind the word vecor\n",
        "\n",
        "#### 'Father'+'Man'+'Woman\"\n",
        "The Cbow model gave the word sorrow, which given the plays and who would be xperiencing this it makes sense. Throne was also included, most of the people in the plays were royalty of sorts so this word also makes sense. Wife, the man/father usually had a wife in these tales, but the las two common and mans dont make sense to inlude. Oerall it did a decent job at describing the idea behind the word vectors\n",
        "\n",
        "skipgram model gave some weird results that i cant really make sense of. Perhaps if we had read the plays they would make sense. Over all we would say it did a good job describing the idea behind the word vectors when taking what it was trained on into consideration\n",
        "\n",
        "Glove Model just gave a couple general terms related to all three. man, woman, mother, father, son. Given the data it was trained on we would say that this was a good description of the word vectors, But weam surprised to not see family in that list of words\n",
        "\n",
        "#### 'Mother'+'Woman'+'man\n",
        "The Cbow model gave some some words that imply that these are related to a sudden (sodiene), sorrow, break in trust. withall means along with everything. With more context to the stories we could judge this better, but overall we will say that it did a good job at capturing the idea behinf the word vectors.\n",
        "\n",
        "\n",
        "The skipgram model provided multiple words that lead us to believe that the women/mothers/men were guilt ridden, had betrayed or been the betrayor, and withall again, so been responsible for everything. Once again, reading the stories would help with understanding wether or not this could be conisdered a good description of the idea behind the word vectors. We would say that this model performed better than the cbow one since it gave us more descriptive words\n",
        "\n",
        "The Glove mode gave some general words related to those three words and did not really capture the idea behind them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kkhF-BxYibi"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "#### Cbow\n",
        "The Cbow model performed great. It had the highet cosine score of the three models with all its cosine scores being over 0.9. but when it came to finding the meaning to one word, we found that it lacked the understanding of the word. When we compared it with multiple words it performed much better but still overall struggled to capture the idea behind word vectors.\n",
        "\n",
        "#### Skipgram\n",
        "The Skipgram model performed interstingly. It had cosine scores worse than the Cbow model but when it cam to actually find the meeaning behind a single word, we found that it outperformed the cbow model. When it came to finding the meaning behind multiple words it began to struggle and give us random words such as titan, land, bears, and withall.\n",
        "\n",
        "#### GloVe\n",
        "The Glove model performed well, its cosine scores were very bad, and when it came to giving descriptions regarding one or more word vectors. it gave a mixed performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXcCZ0vYb-bn"
      },
      "source": [
        "### Imporving the models\n",
        "\n",
        "We believe that including a couple more shakespeare tragidies, such as romeo and juliet, and othello would improve model performance. This would give the models more examples of works of shakespeare to look at and allow them to get a better understanding of the literature.\n",
        "\n",
        "For the glove model, the only idea that we could think of would be to create a transfer learning model using the glove model weights and vocabulary as a base, then training the new model on 2-4 of shakespeare tragedy plays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1M_dwoLj__o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Ot43xMJvepd1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
